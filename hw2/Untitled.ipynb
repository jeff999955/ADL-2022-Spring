{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb6dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from dataset import *\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import same_seeds\n",
    "from model import *\n",
    "\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b976cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def mc_predict(data_loader, model):\n",
    "    model.eval()\n",
    "    relevant = {}\n",
    "    for batch in tqdm(data_loader):\n",
    "        ids, input_ids, attention_masks, token_type_ids, labels = batch\n",
    "        output = model(\n",
    "            input_ids=input_ids.to(args.device),\n",
    "            attention_mask=attention_masks.to(args.device),\n",
    "            token_type_ids=token_type_ids.to(args.device),\n",
    "        )\n",
    "        pred = output.logits.argmax(dim=-1).cpu().numpy()\n",
    "        for _id, _pred in zip(ids, pred):\n",
    "            relevant[_id] = int(_pred)\n",
    "\n",
    "    return relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe95b177",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def qa_predict(args, data_loader, model, n_best = 1):\n",
    "    ret = []\n",
    "    model.eval()\n",
    "    for batch in tqdm(data_loader):\n",
    "        answers = []\n",
    "\n",
    "        ids, inputs = batch\n",
    "        context = inputs[\"context\"][0]\n",
    "        input_ids = inputs[\"input_ids\"].to(args.device)\n",
    "        token_type_ids = inputs[\"token_type_ids\"].to(args.device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(args.device)\n",
    "\n",
    "        qa_output = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            )\n",
    "        \n",
    "        start_logits = qa_output.start_logits.cpu().numpy()\n",
    "        end_logits = qa_output.end_logits.cpu().numpy()\n",
    "        for i in range(len(input_ids)):\n",
    "            start_logit = start_logits[i]\n",
    "            end_logit = end_logits[i]\n",
    "            offsets = inputs[\"offset_mapping\"][i]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    if end_index < start_index:\n",
    "                        continue\n",
    "\n",
    "                    answers.append(\n",
    "                        {\n",
    "                            \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                            \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                        }\n",
    "                    )\n",
    "        best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "        ret.append((ids[0], best_answer[\"text\"]))\n",
    "    return ret  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a89a0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--seed\", type=int, default=5920)\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda\")\n",
    "    parser.add_argument(\n",
    "        \"--data_dir\",\n",
    "        type=Path,\n",
    "        help=\"Directory to the dataset.\",\n",
    "        default=\".\",\n",
    "    )\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"hfl/chinese-macbert-large\")\n",
    "    parser.add_argument(\n",
    "        \"--cache_dir\",\n",
    "        type=str,\n",
    "        help=\"Directory to save the cache file.\",\n",
    "        default=\"./cache\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ckpt_dir\",\n",
    "        type=Path,\n",
    "        help=\"Directory to save the cache file.\",\n",
    "        default=\"/auto/extra/jeff999955\",\n",
    "    )\n",
    "    parser.add_argument(\"--max_len\", type=int, default=512)\n",
    "\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "\n",
    "    args = parser.parse_args(args = [])\n",
    "    return args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f643241",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "641c8f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-macbert-large were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at hfl/chinese-macbert-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "same_seeds(args.seed)\n",
    "accelerator = Accelerator(fp16=True)\n",
    "tags = [\"mc\", \"qa\"]\n",
    "ckpt, config, tokenizer = {}, {}, {}\n",
    "for tag in tags:\n",
    "    ckpt[tag] = torch.load(os.path.join(args.ckpt_dir, f\"{tag}.ckpt\"))\n",
    "    namae = ckpt[tag][\"name\"] \n",
    "    config[tag] = AutoConfig.from_pretrained(namae)\n",
    "    tokenizer[tag] = AutoTokenizer.from_pretrained(\n",
    "        namae, config=config[tag], model_max_length=args.max_len, use_fast=True\n",
    "    )\n",
    "\n",
    "model = MultipleChoiceModel(args, config[\"mc\"], ckpt[\"mc\"][\"name\"])\n",
    "model.load_state_dict(ckpt['mc']['model'])\n",
    "test_set = MultipleChoiceDataset(args, tokenizer[\"mc\"], mode=\"test\")\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    collate_fn=test_set.collate_fn,\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    ")\n",
    "model, test_loader = accelerator.prepare(\n",
    "        model, test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57d6128c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2213/2213 [02:20<00:00, 15.77it/s]\n"
     ]
    }
   ],
   "source": [
    "relevant = mc_predict(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6c576283",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(relevant, \"relevant.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "667d88c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing QA test Data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2213/2213 [00:00<00:00, 582710.45it/s]\n",
      "Some weights of the model checkpoint at hfl/chinese-macbert-large were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at hfl/chinese-macbert-large and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "test_set = QuestionAnsweringDataset(args, tokenizer[\"qa\"], mode=\"test\", relevant = relevant)\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    collate_fn=test_set.collate_fn,\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    ")\n",
    "model = QuestionAnsweringModel(args, config[\"qa\"], ckpt[\"qa\"][\"name\"])\n",
    "model.load_state_dict(ckpt[\"qa\"][\"model\"])\n",
    "model, test_loader = accelerator.prepare(\n",
    "        model, test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75fa7cb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2213/2213 [02:12<00:00, 16.67it/s]\n"
     ]
    }
   ],
   "source": [
    "answers = qa_predict(args, test_loader, model, n_best = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2314bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5e7a923dd6e4ccb8730eb95230e0c908 卡利創立的網際網路檔案館要到什麼時後才開放存取？\n",
      "a2e9cd802197b8f8dfbe235e2761f9ed 哪個國家在歐洲具有重要的戰略意義甚至遠超過了其自身價值?\n",
      "c7c8a85b3f0006d44d86510a22193620 目前所知「義和拳」這一個名詞最早於哪一年時出現?\n",
      "7f4f68726faed6b987e348340a9e6a61 葉門是世界上經濟最落後的國家之一其主要倚賴什麼收入?\n",
      "89908ef5182021a9aec1472c5bbcbd8c 北京地質學院博物館後來演變成哪一個博物館?\n"
     ]
    }
   ],
   "source": [
    "for test in test_set[:5]:\n",
    "    print(test['id'], test['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "291c800e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('5e7a923dd6e4ccb8730eb95230e0c908', '時光機'), ('a2e9cd802197b8f8dfbe235e2761f9ed', '普法茲選侯國'), ('c7c8a85b3f0006d44d86510a22193620', '1779年'), ('7f4f68726faed6b987e348340a9e6a61', '石油收入'), ('89908ef5182021a9aec1472c5bbcbd8c', '中國地質大學逸夫博物館')]\n"
     ]
    }
   ],
   "source": [
    "print(answers[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "231e1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sub.csv', 'w') as f:\n",
    "    print('id,answer', file = f)\n",
    "    for _id, answer in answers:\n",
    "        if '「' in answer and '」' not in answer:\n",
    "            answer += '」'\n",
    "        elif '「' not in answer and '」' in answer:\n",
    "            answer = '「' + answer\n",
    "        if '《' in answer and '》' not in answer:\n",
    "            answer += '》'\n",
    "        elif '《' not in answer and '》' in answer:\n",
    "            answer = '《' + answer\n",
    "        answer = answer.replace(',', '')\n",
    "        print(f'{_id},{answer}', file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc4c0f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2213"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
